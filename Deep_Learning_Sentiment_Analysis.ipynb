{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building a Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports for the code to function\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining global variables which we will measure and use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "global uniq_train_words\n",
    "uniq_train_words=set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of reviews:  <class 'list'>\n",
      "Total number of reviews:  2002\n",
      "Text at index 6:\n",
      "  b'I thought this was a very clunky, uninvolving version of a famous Australian story. Heath Ledger and Orlando Bloom were very good in their roles, and gave their characters some personality; but the whole thing felt forced and mechanical.<br /><br />The beginning could have been a lot more involving; perhaps starting with a shootout, and then flashing back for a recap of how they got there or that sort of thing. And I felt like every scene was routinely predictable and signposted, like a very bad tv soap.<br /><br />I was really looking forward to this movie, and hoping for something a lot better. The only thing I can say in its favour is that it beats the Mick Jagger version, but not by much.'\n",
      "Label of the review at Index 6:  0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "# Read training files\n",
    "reviews_set = load_files(\"aclImdb/data/\")\n",
    "# Lets get training reviews and training labels in sepearate lists\n",
    "reviews, labels = reviews_set.data, reviews_set.target\n",
    "\n",
    "# Let's understand the two lists: reviews (text_train) and their labels (y_train)\n",
    "print(\"Data type of reviews: \",type(reviews))\n",
    "print(\"Total number of reviews: \",len(reviews))\n",
    "print(\"Text at index 6:\\n \", reviews[6])\n",
    "print(\"Label of the review at Index 6: \",labels[6])\n",
    "# 0 for negative and 1 for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_rev, test_rev,train_lbls, test_lbls = train_test_split(reviews, labels, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4>We need to get unique words to determine the vocabulary size based on the words in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Custom function for text noise removal and stemming\n",
    "def noise_removal(text_data, isTraining=True):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    punctuation_list=list(string.punctuation)\n",
    "    # additional punctuation marks\n",
    "    punctuation_list.append(\"--\") \n",
    "    punctuation_list.append(\"''\")\n",
    "    punctuation_list.append(\"``\")\n",
    "    text_data=text_data.replace(\"<br />\", \" \") \n",
    "    # some  stop words\n",
    "    stop_words=['a','an','he','she','it','am','will','have','has','i','you','me','\\'s','``','\\'','(',')','*****','...']\n",
    "    noise_list=punctuation_list +stop_words\n",
    "    # making sure in testing we are keeping only words in training and removing noise in both training and testing\n",
    "    filtered_words=[stemmer.stem(w) for w in nltk.word_tokenize(text_data) if w not in noise_list]\n",
    "       \n",
    "    if isTraining==True:\n",
    "        for w in filtered_words:\n",
    "            uniq_train_words.add(w )\n",
    "    else:# during testing\n",
    "        filtered_words=[w for w in filtered_words if w in uniq_train_words]\n",
    "        \n",
    "    return  \" \".join(filtered_words) # convert the list of tokenize words to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_process(reviews,isTraining=True):\n",
    "    reviews= [noise_removal(text.decode(),isTraining) for text in reviews]\n",
    "    # Integer encode the documents\n",
    "   \n",
    "    vocab_size=len(uniq_train_words)\n",
    "    print(vocab_size)\n",
    "    \n",
    "    print (reviews[0])\n",
    "    encoded_reviews = [one_hot(review, vocab_size) for review in reviews]\n",
    "    \n",
    "    # pad documents to a max length of n words\n",
    "    padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "    return padded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15304\n",
      "Me be of irish origin love thi movi not onli wa the guy hot and funni wa also sincer and honest I love the girl who fell in love with too wa pretti they were such cute coupl the end wa so sad love thi movi although is littl dirti remind of british or irish version of prime If like thi movi should watch prime same stori line young guy fall for older women older women fall for young guy to A lot of path cross in the end the best decis is made or task is complet Do n't anyth els to say without ruin the whole movi all though I thought the french guy wa ugli less appeal to umm if like irish movi I would recommend circl of friend that movi is so good quick quot might not get unless watch well that my dinner ruin lol\n",
      "[ 6751  2978  1739  9645  2030 11927  6353  2605 12127 14276  9613 15277\n",
      "  2793  9645  9330  9703  8512  7867  9214 10253  7612  8891  4124  4848\n",
      " 14968 12333   928  4124  2140 13995 11033  2612  4790 14968 14276  1704\n",
      "  6751 12608 11927 15026 12333  2605  5856  7192 12333 12223  4054  5856\n",
      "  3896  9857   364  4124  1849  8512  6727  9645  1188 12127  1224 11088\n",
      "   880  9703  4984  5856 12742 11851  7948  5891  6751 13649  6751  5635\n",
      "  7048 11171  9703 12333  2605 12127 12476 12333  8802  9703  4124  6294\n",
      " 13995  6727  9645  1188  9703  9364 11069 11726 13864  4758  1447 15228\n",
      "  6002   882 10314  6751  8471  7855 14276  2605 13109 12559     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "train_padded=pre_process(train_rev)\n",
    "print (train_padded[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15304"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total vocabulary size\n",
    "len(uniq_train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>We have completed our pre-processing, it is now time to build the neural network based classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 300, 8)            122432    \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2400)              5762400   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 2401      \n",
      "=================================================================\n",
      "Total params: 5,887,233\n",
      "Trainable params: 5,887,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#from keras import models\n",
    "vocab_size=len(uniq_train_words)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "# Define the embedding matrix dimensions. Each vector is of 8 dimensions and there will be total of vocab_size vectors\n",
    "# The input length (window) is 300 words so the output from embedding layer will be a conactenated (flattened) vector of \n",
    "# 2400 dimensions\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=2400, activation='relu'))\n",
    "model.add(Dropout(0.40))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "# compile the model with optimization algorithm and binary cross entropy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1341/1341 [==============================] - 7s 5ms/step - loss: 4.6362e-07 - acc: 1.0000\n",
      "Epoch 2/20\n",
      "1341/1341 [==============================] - 7s 5ms/step - loss: 4.7121e-07 - acc: 1.0000\n",
      "Epoch 3/20\n",
      "1341/1341 [==============================] - 7s 5ms/step - loss: 4.7417e-07 - acc: 1.0000\n",
      "Epoch 4/20\n",
      "1341/1341 [==============================] - 7s 6ms/step - loss: 4.3735e-07 - acc: 1.0000\n",
      "Epoch 5/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 4.0417e-07 - acc: 1.0000\n",
      "Epoch 6/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 4.0564e-07 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 4.0596e-07 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 4.0692e-07 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 3.9240e-07 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 3.6447e-07 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 3.6446e-07 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 3.6938e-07 - acc: 1.0000A: 0s - loss: 3.6875e-07 - acc: 1.000\n",
      "Epoch 13/20\n",
      "1341/1341 [==============================] - 9s 6ms/step - loss: 3.4136e-07 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "1341/1341 [==============================] - 9s 6ms/step - loss: 3.4610e-07 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "1341/1341 [==============================] - 8s 6ms/step - loss: 3.3626e-07 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "1341/1341 [==============================] - 9s 6ms/step - loss: 3.2293e-07 - acc: 1.0000A: 8s - loss: 2.7 - ETA: 3s - loss: 2.98\n",
      "Epoch 17/20\n",
      "1341/1341 [==============================] - 9s 7ms/step - loss: 3.2200e-07 - acc: 1.0000A: 1s - loss: 3.2582e-07 - ac\n",
      "Epoch 18/20\n",
      "1341/1341 [==============================] - 9s 7ms/step - loss: 2.9981e-07 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "1341/1341 [==============================] - 9s 7ms/step - loss: 2.9303e-07 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "1341/1341 [==============================] - 10s 7ms/step - loss: 2.8069e-07 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1fc01760550>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model...I am trying with batch_size=30, you can delete it for default batch \n",
    "#size or change it to a bigger number\n",
    "model.fit(train_padded, train_lbls, epochs=20, batch_size=30,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15304\n",
      "unwatch you ca n't even make past the first three minut and thi is come from huge adam sandler fan 1\n",
      "[ 1259 11823  5178  9396  7323  1571  9703  2883 12365  1869  9613 14058\n",
      " 12602  4124  8195  6867  7985  4124   765  5307  1259 12333  7928  1473\n",
      "  9703 14673  4124  5456  9703  1231  3768  3357  1739  5245 14904 11621\n",
      "  9977  6751  1856  3768 13362 14276 14716 13077  9232 12859 12173 14968\n",
      "  2281 12519  6867  7985  7439  9613  4621  5595 14167 14699  7300  2593\n",
      "  6253  3768  7439  1231 11069  9943  4124  7441 11973  7439  9613 10253\n",
      "  9703  5595  7964  1435   488  8437  1856  3768 14029  4013  1224  6253\n",
      "  9943  4124  7322  6629 10270  4352  9645  4431 10492  6002 12519  7928\n",
      " 10138  2141 14699  2883 12365 10665   166  9645 12200 10013   540  8560\n",
      " 14276  2605  6002 12411  2436  4124  8624  3433  3042  2927  1428  9543\n",
      "  2605  9645 10678  8195 14673   166 14297  5429 14968  1413  7814  7700\n",
      "  4124 11863 12333  8629  8104 14968  3744  5178  9396  7323  8011  7894\n",
      "  9613 14029 10859 10138 14759     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#pre process like training set\n",
    "test_padded=pre_process(test_rev, isTraining=False)\n",
    "print (test_padded[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661/661 [==============================] - 0s 682us/step\n",
      "Accuracy: 71.835098\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(test_padded, test_lbls, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661/661 [==============================] - 0s 218us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.68       341\n",
      "           1       0.00      0.00      0.00       320\n",
      "\n",
      "    accuracy                           0.52       661\n",
      "   macro avg       0.26      0.50      0.34       661\n",
      "weighted avg       0.27      0.52      0.35       661\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shary\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "predictions = model.predict(test_padded, batch_size=100, verbose=1)\n",
    "predictions_bool = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(classification_report(test_lbls, predictions_bool))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
